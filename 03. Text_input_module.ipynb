{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import openai\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from image_utils import fetch_clip, draw_images\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# pandas dataframe display\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"attribute_specific.csv\")\n",
    "new_df = pd.read_csv(\"clothes_final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsert to pineconeDB!!\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "# Check the number of indexes\n",
    "# index_list = pc.list_indexes().indexes\n",
    "\n",
    "# index description\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import get_single_text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"black dog\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=5,\n",
    "    # filter={\"category\": {\"$eq\": \"dress\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First gateway\n",
    "- Determines whether the topic is related to fashion.\n",
    "- A semantic router can also be used, but it has limitations because the router needs to be specified (27 classes).\n",
    "- The section that first receives user input.\n",
    "    - -> Use openai chat completion to determine whether the user's text input is content that we should actually receive and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Literal\n",
    "\n",
    "from llama_index.program.openai import OpenAIPydanticProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gateway_prompt = \"\"\"Input text : {text_input}\n",
    "Using the input text, do the following\n",
    "- clothes_topic : Determine whether the text describes a clothes. The output should be a python boolean.\n",
    "- fashion_item : Determine whether it mentions a specific fashion items such as boots or shirt, umbrella etc. The output should be a python boolean.\n",
    "\"\"\"\n",
    "\n",
    "class first_gateway(BaseModel):\n",
    "    \"\"\"Data model to determine whether the text describes a fashion type or clothes type.\"\"\"\n",
    "    clothes_topic: bool\n",
    "    fashion_item: bool\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=first_gateway, prompt_template_str=first_gateway_prompt, llm=llm,verbose=True\n",
    ")\n",
    "\n",
    "output = program(\n",
    "    text_input=\"street fashion\"\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['bohemian style boots', 'old school', 'a cup of tea', 'umbrella', 'a black hat', 'suit and tie', 'wedding apparel',\n",
    "          'blue fashion socks', 'car']:\n",
    "    print(t)\n",
    "    print(program(\n",
    "                text_input=t\n",
    "            ))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second gateway\n",
    "- A process of transforming user input into the desired input format.\n",
    "- Restrict the search space by forcing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"black jacket\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"black jacket\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_gateway_prompt = \"\"\"Input text : {text_input}.\n",
    "Using the input text, do the following.\n",
    "\n",
    "First, divide the items listed in the sentence, ensuring that descriptive words for each item are kept together during the separation.\n",
    "Second, for each item listed, do the following :\n",
    "    - Categorize the clothes type mentioned from the input.\n",
    "        - From the options below, choose the clothes type mentioned. : \n",
    "            'pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt',\n",
    "            'dress', 'shoe', 'glasses', 'skirt', 'bag, wallet', 'belt',\n",
    "            'headband, head covering, hair accessory', 'sock', 'hat', 'watch',\n",
    "            'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf',\n",
    "            'coat', 'vest', 'umbrella', 'cardigan', 'cape', 'jumpsuit',\n",
    "            'leg warmer'\n",
    "        - a suit is part of jacket\n",
    "        - If none of the above is mentioned, say \"None\"\n",
    "    - Refine the text into a comma-separated string of attributes\n",
    "        -  as an example, the text 'casual, urban-inspired jacket with bold graphics and loose-fitting designs'\n",
    "        would be converted to 'casual, urban-inspired, jacket, bold graphics, loose-fit'.\n",
    "        - another example, the text 'color Pink, - silhouette Straight, - silhouette_fit Loose'\n",
    "        would be converted to 'color pink, silhouette Straight, silhouette_fit Loose'.\n",
    "        - do not hesitate to repeat the modifiers for each item.\n",
    "The output should be in English.\n",
    "\"\"\"\n",
    "\n",
    "class second_gateway_list(BaseModel):\n",
    "    \"\"\"Data model to categorize the clothing type, and refine text into a specific format.\"\"\"\n",
    "    clothes_type: Literal['pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt',\n",
    "                            'dress', 'shoe', 'glasses', 'skirt', 'bag, wallet', 'belt',\n",
    "                            'headband, head covering, hair accessory', 'sock', 'hat', 'watch',\n",
    "                            'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf',\n",
    "                            'coat', 'vest', 'umbrella', 'cardigan', 'cape', 'jumpsuit',\n",
    "                            'leg warmer', \"None\"]\n",
    "    refined_text: str\n",
    "\n",
    "class second_gateway(BaseModel):\n",
    "    \"\"\"Data model to list items.\"\"\"\n",
    "    items: List[second_gateway_list]\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=second_gateway, prompt_template_str=second_gateway_prompt, llm=llm, verbose=False\n",
    ")\n",
    "\n",
    "output = program(\n",
    "    text_input=\"street fashion boots\"\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['bohemian style pants', 'street fashion', 'a black hat', 'suit and tie', 'wedding apparel',\n",
    "          'blue fashion socks', 'old school', 'umbrella']:\n",
    "    print(t)\n",
    "    print(program(\n",
    "                text_input=t\n",
    "            ))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_gateway_prompt = \"\"\"Input text : {text_input}.\n",
    "Using the input text, do the following.\n",
    "    - Refine the text into a comma-separated string of attributes\n",
    "        -  as an example, the text 'casual, urban-inspired jacket with bold graphics and loose-fitting designs'\n",
    "        would be converted to 'casual, urban-inspired, jacket, bold graphics, loose-fit'\n",
    "        - do not hesitate to repeat the modifiers for each item.\n",
    "\"\"\"\n",
    "\n",
    "class third_gateway_list(BaseModel):\n",
    "    \"\"\"Data model to reformat an input text.\"\"\"\n",
    "    refined_text: str\n",
    "\n",
    "class third_gateway(BaseModel):\n",
    "    \"\"\"Data model to list items.\"\"\"\n",
    "    items: List[third_gateway_list]\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=third_gateway, prompt_template_str=third_gateway_prompt, llm=llm, verbose=False\n",
    ")\n",
    "\n",
    "output = program(\n",
    "    text_input=\"bohemian style clothes\"\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['bohemian style pants', 'street fashion', 'a black hat', 'suit and tie', 'wedding apparel',\n",
    "          'blue fashion socks', 'old school', 'umbrella', \"I want a black jacket with gold zippers\"]:\n",
    "    print(t)\n",
    "    print(program(\n",
    "                text_input=t\n",
    "            ))\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input processing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_first_gateway(input_text, llm, verbose=False):\n",
    "    first_gateway_prompt = \"\"\"Input text : {text_input}\n",
    "    Using the input text, do the following\n",
    "    - clothes_topic : Determine whether the subject it is related to fashion or clothes. The output should be a python boolean.\n",
    "    - Determine whether it mentions a specific fashion items such as boots or shirt, umbrella etc. The output should be a python boolean.\n",
    "    \"\"\"\n",
    "    \n",
    "    class first_gateway(BaseModel):\n",
    "        \"\"\"Data model to determine whether the text is related to clothes.\"\"\"\n",
    "        clothes_topic: bool\n",
    "        fashion_item: bool\n",
    "\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        output_cls=first_gateway, prompt_template_str=first_gateway_prompt, llm=llm,verbose=verbose\n",
    ")\n",
    "\n",
    "    output = program(\n",
    "        text_input=input_text\n",
    "    )\n",
    "\n",
    "    return output.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_second_gateway(text_input, llm, verbose=False):\n",
    "    second_gateway_prompt = \"\"\"Input text : {text_input}.\n",
    "    Using the input text, do the following.\n",
    "\n",
    "    First, divide the items listed in the sentence, ensuring that descriptive words for each item are kept together during the separation.\n",
    "    Second, for each item listed, do the following :\n",
    "        - Categorize the clothes type mentioned from the input.\n",
    "            - From the options below, choose the clothes type mentioned. : \n",
    "                'pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt',\n",
    "                'dress', 'shoe', 'glasses', 'skirt', 'bag, wallet', 'belt',\n",
    "                'headband, head covering, hair accessory', 'sock', 'hat', 'watch',\n",
    "                'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf',\n",
    "                'coat', 'vest', 'umbrella', 'cardigan', 'cape', 'jumpsuit',\n",
    "                'leg warmer'\n",
    "            - a suit is part of jacket\n",
    "            - If none of the above is mentioned, say \"None\"\n",
    "        - Refine the text into a comma-separated string of attributes\n",
    "            -  as an example, the text 'casual, urban-inspired jacket with bold graphics and loose-fitting designs'\n",
    "            would be converted to 'casual, urban-inspired, jacket, bold graphics, loose-fit'.\n",
    "            - another example, the text 'color Pink, - silhouette Straight, - silhouette_fit Loose'\n",
    "            would be converted to 'color pink, silhouette Straight, silhouette_fit Loose'.\n",
    "            - do not hesitate to repeat the modifiers for each item.\n",
    "    The output should be in English.\n",
    "    \"\"\"\n",
    "\n",
    "    class second_gateway_list(BaseModel):\n",
    "        \"\"\"Data model to categorize the clothing type, and refine text into a specific format.\"\"\"\n",
    "        clothes_type: Literal['pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt',\n",
    "                            'dress', 'shoe', 'glasses', 'skirt', 'bag, wallet', 'belt',\n",
    "                            'headband, head covering, hair accessory', 'sock', 'hat', 'watch',\n",
    "                            'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf',\n",
    "                            'coat', 'vest', 'umbrella', 'cardigan', 'cape', 'jumpsuit',\n",
    "                            'leg warmer']\n",
    "        refined_text: str\n",
    "\n",
    "    class second_gateway(BaseModel):\n",
    "        \"\"\"Data model to list items.\"\"\"\n",
    "        items: List[second_gateway_list]\n",
    "\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        output_cls=second_gateway, prompt_template_str=second_gateway_prompt, llm=llm, verbose=verbose\n",
    ")\n",
    "\n",
    "    output = program(\n",
    "        text_input=text_input\n",
    "    )\n",
    "\n",
    "    return output.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_third_gateway(text_input, llm, verbose=False):\n",
    "    \n",
    "    third_gateway_prompt = \"\"\"Input text : {text_input}.\n",
    "    Using the input text, do the following.\n",
    "        - Refine the text into a comma-separated string of attributes\n",
    "            -  as an example, the text 'casual, urban-inspired jacket with bold graphics and loose-fitting designs'\n",
    "            would be converted to 'casual, urban-inspired, jacket, bold graphics, loose-fit'\n",
    "            - do not hesitate to repeat the modifiers for each item.\n",
    "    \"\"\"\n",
    "\n",
    "    class third_gateway_list(BaseModel):\n",
    "        \"\"\"Data model to reformat an input text.\"\"\"\n",
    "        refined_text: str\n",
    "\n",
    "    class third_gateway(BaseModel):\n",
    "        \"\"\"Data model to list items.\"\"\"\n",
    "        items: List[third_gateway_list]\n",
    "\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        output_cls=third_gateway, prompt_template_str=third_gateway_prompt, llm=llm, verbose=verbose\n",
    ")\n",
    "\n",
    "    output = program(\n",
    "        text_input=text_input\n",
    "    )\n",
    "    return output.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with the Search module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- vans shoes\n",
    "\n",
    "=> {\"shoes\":'vans shoes'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def text_search(index, items_dict, model, tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=False):\n",
    "    search_results = dict()\n",
    "    for item in items_dict['items']:\n",
    "        text_emb = get_single_text_embedding(item['refined_text'], model, tokenizer)\n",
    "        if hybrid:\n",
    "            sparse_vector = gen_sparse_vector(item['refined_text'], splade_model, splade_tokenizer)\n",
    "        else:\n",
    "            sparse_vector=None\n",
    "        \n",
    "        if 'clothes_type' in list(item.keys()):\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            filter={\"category\": {\"$eq\": item['clothes_type']}},\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results[item['clothes_type']] = search_result\n",
    "        else:\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results['all'] = search_result\n",
    "    return search_results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import text_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsert to pineconeDB!!\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "# Check the number of indexes\n",
    "# index_list = pc.list_indexes().indexes\n",
    "\n",
    "# index description\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "splade_model = Splade(splade_model_id, agg='max')\n",
    "splade_model.to('cpu')\n",
    "splade_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import fetch_clip\n",
    "# fetch CLIP model\n",
    "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Street fashioned boots and jacket, with colorful socks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gateway_output = pass_first_gateway(example_text, llm)\n",
    "first_gateway_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_gateway_output = pass_second_gateway(example_text, llm)\n",
    "second_gateway_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_third_gateway(example_text, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import get_single_text_embedding, gen_sparse_vector\n",
    "from image_utils import draw_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define user journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Street fashioned boots and jacket, with colorful socks\"\n",
    "\n",
    "first_gateway_output = pass_first_gateway(example_text, llm)\n",
    "print(\"first_gateway_output : \", first_gateway_output)\n",
    "if (first_gateway_output['clothes_topic']):\n",
    "    print(\"Passed the first gateway. Moving on to the second gateway...\")\n",
    "    if (not first_gateway_output['fashion_item']):\n",
    "        \n",
    "        print(\"However, specific item is not found. Searching the whole database.\")\n",
    "        gateway_output = pass_third_gateway(example_text, llm)\n",
    "        filter=False\n",
    "    else:\n",
    "        gateway_output = pass_second_gateway(example_text, llm)\n",
    "        filter=True\n",
    "    search_results = text_search(index, gateway_output, model, tokenizer, splade_model, splade_tokenizer, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = dict()\n",
    "for k,v in search_results.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"street fashion\"\n",
    "\n",
    "first_gateway_output = pass_first_gateway(example_text, llm)\n",
    "\n",
    "if (first_gateway_output['clothes_topic']):\n",
    "    print(\"Passed the first gateway. Moving on to the second gateway...\")\n",
    "    if (not first_gateway_output['fashion_item']):\n",
    "        \n",
    "        print(\"However, specific item is not found. Searching the whole database.\")\n",
    "        gateway_output = pass_third_gateway(example_text, llm)\n",
    "        filter=False\n",
    "    else:\n",
    "        gateway_output = pass_second_gateway(example_text, llm)\n",
    "        filter=True\n",
    "    search_results = text_search(index, gateway_output, model, tokenizer, splade_model, splade_tokenizer, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = dict()\n",
    "for k,v in search_results.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_query_transformer(text_input):\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "    #### Determines whether the text is related to fashion\n",
    "    first_gateway_output = pass_first_gateway(text_input, llm)\n",
    "    print(first_gateway_output)\n",
    "\n",
    "    if (first_gateway_output['clothes_topic']):\n",
    "        # print(\"Passed the first gateway. Moving on to the second gateway...\")\n",
    "        if (not first_gateway_output['fashion_item']):\n",
    "            # print(\"However, specific item is not found. Searching the whole database.\")\n",
    "            gateway_output = pass_third_gateway(text_input, llm)\n",
    "        else:\n",
    "            done=False\n",
    "            while not done:\n",
    "                try:\n",
    "                    gateway_output = pass_second_gateway(text_input, llm)\n",
    "                    done=True\n",
    "                except:\n",
    "                    continue\n",
    "    else:\n",
    "        return None\n",
    "    return gateway_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(index, items_dict, model, tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=False):\n",
    "    search_results = dict()\n",
    "    for item in items_dict['items']:\n",
    "        text_emb = get_single_text_embedding(item['refined_text'], model, tokenizer)\n",
    "        if hybrid:\n",
    "            sparse_vector = gen_sparse_vector(item['refined_text'], splade_model, splade_tokenizer)\n",
    "        else:\n",
    "            sparse_vector=None\n",
    "        \n",
    "        if 'clothes_type' in list(item.keys()):\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            filter={\"category\": {\"$eq\": item['clothes_type']}},\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results[item['clothes_type']] = search_result\n",
    "        else:\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results['all'] = search_result\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "- Text unrelated to fashion -> None\n",
    "- Fashion style text -> hybrid search\n",
    "- Fashion item text -> apply filter & hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"a fluffy cat\"\n",
    "sparse_query = gen_sparse_vector(input, splade_model, splade_tokenizer)\n",
    "\n",
    "# Check and convert the input\n",
    "text_query = fashion_query_transformer(input)\n",
    "\n",
    "if text_query:\n",
    "    # search\n",
    "    result = text_search(index, text_query, model, tokenizer, splade_model, splade_tokenizer, top_k=10)\n",
    "\n",
    "    # Get the paths of the images\n",
    "    paths = dict()\n",
    "    for k,v in result.items():\n",
    "        paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "    # Show the images\n",
    "    for k,v in paths.items():\n",
    "        print(k)\n",
    "        draw_images([Image.open(i) for i in v])\n",
    "else:\n",
    "    print(\"This text is not related to fashion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"vans shoes with formal suit and a red tie for a wedding\"\n",
    "text_query = fashion_query_transformer(input)\n",
    "\n",
    "if text_query:\n",
    "    print(text_query)\n",
    "    # search\n",
    "    sparse_query = gen_sparse_vector(input, splade_model, splade_tokenizer)\n",
    "    result = text_search(index, text_query, model, tokenizer, splade_model, splade_tokenizer, top_k=10)\n",
    "\n",
    "    # Get the paths of the images\n",
    "    paths = dict()\n",
    "    for k,v in result.items():\n",
    "        paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "    # Show the images\n",
    "    for k,v in paths.items():\n",
    "        print(k)\n",
    "        draw_images([Image.open(i) for i in v])\n",
    "else:\n",
    "    print(text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"creative fashion\"\n",
    "\n",
    "# Check and convert the input\n",
    "text_query = fashion_query_transformer(input)\n",
    "\n",
    "if text_query:\n",
    "    # search\n",
    "    result = text_search(index, text_query, model, tokenizer, splade_model, splade_tokenizer, top_k=10)\n",
    "\n",
    "    # Get the paths of the images\n",
    "    paths = dict()\n",
    "    for k,v in result.items():\n",
    "        paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "    # Show the images\n",
    "    for k,v in paths.items():\n",
    "        print(k)\n",
    "        draw_images([Image.open(i) for i in v])\n",
    "else:\n",
    "    print(text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}