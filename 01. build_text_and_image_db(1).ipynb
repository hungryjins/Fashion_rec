{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from image_utils import crop_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clothes_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bbox'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(string, encap_type=\"()\"):\n",
    "    return [int(num) for num in string.strip(encap_type).split(', ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reading for the first time, it's recognized as a string value instead of a list in the pandas dataframe, so conversion is necessary\n",
    "df['bbox'] = [listify(i) for i in df['bbox']]\n",
    "df['bbox_big'] = [listify(i) for i in df['bbox_big']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents - CLIP embeddings\n",
    "\n",
    "- Save cropped images for each product locally\n",
    "- Generate CLIP embeddings\n",
    "\n",
    "## 1. Bounding box EDA\n",
    "\n",
    "## 2. Crop each entity based on the bounding box\n",
    "\n",
    "## 3. Resize cropped images according to each item and save locally\n",
    "\n",
    "## 4. Embedding using CLIP\n",
    "\n",
    "- Fine-tuned CLIP\n",
    "- Text & image represented in a single embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bounding box EDA\n",
    "- What is the 'size' of the products in each image?\n",
    "- Similarity is also affected by the size of the image, making it an important factor.\n",
    "- Therefore, it is important that all images belonging to one category are represented in the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = crop_bbox(img, df['bbox'][218])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")\n",
    "cropped = crop_bbox(img, df['bbox'][223])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in df['supercategory'].unique():\n",
    "    tmp = df.loc[df['supercategory']==cat]\n",
    "    print(cat)\n",
    "    print(tmp['name'].unique())\n",
    "    print(\"Area : {}, width : {}, height : {}.format(np.median(tmp['area']), np.median(tmp['width']), np.median(tmp['height'])))\\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category has its own image characteristics.\n",
    "- lower body has an average width of 410 and height of 540\n",
    "- upper body has a longer height ratio than lower body\n",
    "- wholebody has an even longer height ratio\n",
    "- waist is wider than it is high\n",
    "- arms and hands have a similar width-to-height ratio and are generally small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crop each entity based on the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = {\"lowerbody\":[420, 540],\n",
    "        \"upperbody\":[500, 700],\n",
    "        \"wholebody\":[480, 880],\n",
    "        \"legs and feet\":[100, 150],\n",
    "        \"head\":[150, 100],\n",
    "        \"others\":[200, 350],\n",
    "        \"waist\":[200, 100],\n",
    "        \"arms and hands\":[75, 75],\n",
    "        \"neck\":[120, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/00000663ed1ff0c4e0132b9b9ac53f6e.jpg\")\n",
    "cropped = crop_bbox(img, df['bbox_big'][0])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def resize_img(image, standard_size, category):\n",
    "    w, h = image.size\n",
    "    img_size = w*h\n",
    "\n",
    "    new_width, new_height = standard_size[category]\n",
    "    new_size = new_width * new_height\n",
    "\n",
    "    if img_size >= new_size:\n",
    "        # For downsizing\n",
    "        downsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        return downsized_image\n",
    "    else:\n",
    "        # For upsizing\n",
    "        upsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        upsized_image = upsized_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "        return upsized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_img(cropped, size, df['supercategory'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resize cropped images according to each item and save locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"imaterialist-fashion-2020-fgvc7/train\"\n",
    "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for image_name in tqdm(df['ImageId'].unique()):\n",
    "    # dataframe related to one image\n",
    "    tmp = df.loc[df['ImageId']==image_name]\n",
    "    tmp = tmp.reset_index().rename(columns={\"index\":\"entity_id\"})\n",
    "    image = Image.open(os.path.join(base_path, image_name+\".jpg\"))\n",
    "    # crop products within each image -> save locally\n",
    "    for idx, row in tmp.iterrows():\n",
    "        cropped_img = crop_bbox(image, row['bbox_big'])\n",
    "        resized_img = resize_img(cropped_img, size, row['supercategory'])\n",
    "        resized_img.save(os.path.join(cropped_path, image_name + \"_\" + str(row['entity_id']) + \".jpg\"))\n",
    "\n",
    "    new_df = pd.concat([new_df, tmp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"clothes_final2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
    "\n",
    "new_df['bbox'] = [listify(i, \"[]\") for i in new_df['bbox']]\n",
    "new_df['bbox_big'] = [listify(i, \"[]\") for i in new_df['bbox_big']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding using CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-trained CLIP model using a fashion dataset\n",
    "- The CLIP model uses <image>-<caption> pairs as input data, implementing both in a single, unified embedding space.\n",
    "- Therefore, a model fine-tuned using <fashion image>-<fashion caption> pairs is suitable for the current project's purpose.\n",
    "- Planning to measure embedding ranking using the dot product.\n",
    "```json\n",
    "\"FashionCLIP performs the dot product between the input caption embedding and each image vector embedding\"\n",
    "\n",
    "\"The text used is a concatenation of the highlight (e.g., \"stripes\", \"long sleeves\", \"Armani\") and short description (\"80s styled t-shirt\")) available in the Farfetch dataset.\"\n",
    "```\n",
    "\n",
    "![Fine-tune training data](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-022-23052-9/MediaObjects/41598_2022_23052_Fig3_HTML.png?as=webp, \"Fine-tune training data\")\n",
    "\n",
    "(Source: Contrastive language and vision learning of general fashion concepts)\n",
    "\n",
    "- hugging face : https://huggingface.co/patrickjohncyh/fashion-clip\n",
    "- paper : https://www.nature.com/articles/s41598-022-23052-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-CLIP VS CLIP Performance Difference\n",
    "\n",
    "https://www.nature.com/articles/s41598-022-23052-9/tables/1\n",
    "\n",
    "- HIT@5 = (Number of related products in the top 5 search results) / (Total number of related products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths of cropped images\n",
    "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
    "\n",
    "images = list(os.walk(cropped_path))[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image embeddings from CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import extract_img_features\n",
    "\n",
    "img_emb = extract_img_features(img, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See `01.Create_image_embeddings.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text embeddings from CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "\n",
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_text_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    # convert the embeddings to numpy array\n",
    "    embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
    "    return embedding_as_np.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/3bccf2e618d8f5f51442037ad3c8d4fb.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fashion fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"The text used is a concatenation of the highlight (e.g., \"stripes\", \"long sleeves\", \"Armani\") and short description (\"80s styled t-shirt\")) available in the Farfetch dataset.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = extract_img_features(img, processor, model)\n",
    "\n",
    "sample_texts = ['tshirt', \"formal suit and tie\", \n",
    "                'a woman', \"a lion in a cage\", \"black top short sleeves\",\n",
    "                'black shirt with check patterns, topwear', 'iphone']\n",
    "\n",
    "sample_texts_emb = get_single_text_embedding(sample_texts, model, tokenizer)\n",
    "\n",
    "sims = cosine_similarity(img_emb.cpu().detach().numpy(), sample_texts_emb)\n",
    "# Although we plan to use the dot product in the future,\n",
    "print(\"Similarity with image\")\n",
    "for t, s in zip(sample_texts, sims[0]):\n",
    "    print(\"{}: {}.format(t, s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb.cpu().detach().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dot product')\n",
    "for text, s in zip(sample_texts, sample_texts_emb):\n",
    "    sim = np.dot(img_emb.cpu().detach().numpy()[0], np.array(s))\n",
    "    print(text, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "with open('img_embeddings_fashion_fine_tuned.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Convert each line to a dictionary\n",
    "        embedding_dict = json.loads(line.strip())\n",
    "        \n",
    "        # Convert the list back to a NumPy array if necessary\n",
    "        for img_name, emb_list in embedding_dict.items():\n",
    "            embeddings[img_name] = np.array(emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in embeddings.items():\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}