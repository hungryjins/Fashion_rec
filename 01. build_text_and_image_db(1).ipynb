{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungryjins/Fashion_rec/blob/main/01.%20build_text_and_image_db(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDEzg-C3HTc-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from image_utils import crop_bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yACdmEvAHTdD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"clothes_final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx9AP4yRHTdE"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLPifrMGHTdE"
      },
      "outputs": [],
      "source": [
        "df['bbox'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgDskEehHTdF"
      },
      "outputs": [],
      "source": [
        "def listify(string, encap_type=\"()\"):\n",
        "    return [int(num) for num in string.strip(encap_type).split(', ')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3CAWxzYHTdF"
      },
      "outputs": [],
      "source": [
        "# It needs to be converted because pandas dataframe recognizes it as a string, not a list, when reading for the first time.\n",
        "df['bbox'] = [listify(i) for i in df['bbox']]\n",
        "df['bbox_big'] = [listify(i) for i in df['bbox_big']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WdAjO7oHTdG"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfUnynigHTdH"
      },
      "source": [
        "üìå Table of Contents ‚Äì CLIP Embeddings\n",
        "Save cropped images of each product locally\n",
        "Generate CLIP embeddings\n",
        "1. Exploratory Data Analysis of Bounding Boxes\n",
        "\n",
        "2. Crop each entity based on its bounding box\n",
        "\n",
        "3. Resize the cropped images by category and save them locally\n",
        "\n",
        "4. Generate embeddings using CLIP\n",
        "\n",
        "Use a fine-tuned CLIP model\n",
        "Represent both text and images in a unified embedding space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfuHmpKNHTdL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kvelANPZJGWe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3757c5d9"
      },
      "source": [
        "## 1. Bounding box EDA\n",
        "- What is the 'size' of the products in each image?\n",
        "- Similarity is an important factor because it is affected by image size.\n",
        "- Therefore, it is important that images belonging to one category are all represented in the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42q3HMgyHTdL"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbP8HoCkHTdM"
      },
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym0b3dUgHTdN"
      },
      "outputs": [],
      "source": [
        "df.loc[218]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMPcg_-LHTdO"
      },
      "outputs": [],
      "source": [
        "cropped = crop_bbox(img, df['bbox'][218])\n",
        "cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReIiEwdQHTdP"
      },
      "outputs": [],
      "source": [
        "df.loc[223]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_omC7tC6HTdQ"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")\n",
        "cropped = crop_bbox(img, df['bbox'][223])\n",
        "cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrz59wO9HTdQ"
      },
      "outputs": [],
      "source": [
        "for cat in df['supercategory'].unique():\n",
        "    tmp = df.loc[df['supercategory']==cat]\n",
        "    print(cat)\n",
        "    print(tmp['name'].unique())\n",
        "    print(\"Area : {}, width : {}, height : {}\".format(np.median(tmp['area']), np.median(tmp['width']), np.median(tmp['height'])))\n",
        "    print(\"-\"*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMiKleh2HTdQ"
      },
      "source": [
        "\n",
        "Each category has its own image characteristics\n",
        "- lower body is average 410 horizontally, 540 vertically- upper body is longer vertically than lower body- wholebody is longer vertically than that\n",
        "- waist is longer horizontally than vertically\n",
        "- arms and hands have similar horizontal and vertical ratios and are generally small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYvAYPgHTdR"
      },
      "source": [
        "## 2. Crop each entity based on the bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqhHhlYoHTdR"
      },
      "outputs": [],
      "source": [
        "size = {\"lowerbody\":[420, 540],\n",
        "        \"upperbody\":[500, 700],\n",
        "        \"wholebody\":[480, 880],\n",
        "        \"legs and feet\":[100, 150],\n",
        "        \"head\":[150, 100],\n",
        "        \"others\":[200, 350],\n",
        "        \"waist\":[200, 100],\n",
        "        \"arms and hands\":[75, 75],\n",
        "        \"neck\":[120, 200]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COEtH281HTdS"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUiKQ4hHTdS"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/00000663ed1ff0c4e0132b9b9ac53f6e.jpg\")\n",
        "cropped = crop_bbox(img, df['bbox_big'][0])\n",
        "cropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z54Ye2pcHTdT"
      },
      "source": [
        "#### image resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY91ysB7HTdT"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "\n",
        "def resize_img(image, standard_size, category):\n",
        "    w, h = image.size\n",
        "    img_size = w*h\n",
        "\n",
        "    new_width, new_height = standard_size[category]\n",
        "    new_size = new_width * new_height\n",
        "\n",
        "    if img_size >= new_size:\n",
        "        # For downsizing\n",
        "        downsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "        return downsized_image\n",
        "    else:\n",
        "        # For upsizing\n",
        "        upsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "        upsized_image = upsized_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
        "        return upsized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqdbGdMTHTdT"
      },
      "outputs": [],
      "source": [
        "resize_img(cropped, size, df['supercategory'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCBoZHBTHTdU"
      },
      "source": [
        "## 3. Resize and save cropped images locally according to each item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwKbk1QkHTdU"
      },
      "outputs": [],
      "source": [
        "base_path = \"imaterialist-fashion-2020-fgvc7/train\"\n",
        "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "for image_name in tqdm(df['ImageId'].unique()):\n",
        "    # Ìïú Ïù¥ÎØ∏ÏßÄÏôÄ Í¥ÄÎ†®Îêú dataframe\n",
        "    tmp = df.loc[df['ImageId']==image_name]\n",
        "    tmp = tmp.reset_index().rename(columns={\"index\":\"entity_id\"})\n",
        "    image = Image.open(os.path.join(base_path, image_name+\".jpg\"))\n",
        "    # Í∞Å Ïù¥ÎØ∏ÏßÄ ÎÇ¥Ïóê ÏûàÎäî ÏÉÅÌíàÎì§ÏùÑ crop -> local save\n",
        "    for idx, row in tmp.iterrows():\n",
        "        cropped_img = crop_bbox(image, row['bbox_big'])\n",
        "        resized_img = resize_img(cropped_img, size, row['supercategory'])\n",
        "        resized_img.save(os.path.join(cropped_path, image_name + \"_\" + str(row['entity_id']) + \".jpg\"))\n",
        "\n",
        "    new_df = pd.concat([new_df, tmp], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWr-bWnYHTdV"
      },
      "outputs": [],
      "source": [
        "# new_df.to_csv(\"clothes_final2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDTzbzqlHTdV"
      },
      "outputs": [],
      "source": [
        "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
        "\n",
        "new_df['bbox'] = [listify(i, \"[]\") for i in new_df['bbox']]\n",
        "new_df['bbox_big'] = [listify(i, \"[]\") for i in new_df['bbox_big']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0n73cPbHTdV"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_lBn3o-HTdW"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXJG5BqvHTdY"
      },
      "source": [
        "## 4.Embedding using CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ1HozF1HTdb"
      },
      "source": [
        "- fashion dataset used for pretraining the CLIP model\n",
        "- The CLIP model uses <image>-<text> pairs as input data, representing both in a single embedding space.\n",
        "- Therefore, a model fine-tuned using <fashion image>-<fashion caption> pairs is suitable for the current project purpose.\n",
        "- Dot product will be used to measure embedding ranking.\n",
        "```json\n",
        "\"FashionCLIP performs the dot product between the input caption embedding and each image vector embedding\"\n",
        "\n",
        "\"The text used is a concatenation of the highlight (e.g., ‚Äústripes‚Äù, ‚Äúlong sleeves‚Äù, ‚ÄúArmani‚Äù) and short description (‚Äú80s styled t-shirt‚Äù)) available in the Farfetch dataset.\"\n",
        "```\n",
        "\n",
        "![Fine-tune ÌõàÎ†® Îç∞Ïù¥ÌÑ∞](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-022-23052-9/MediaObjects/41598_2022_23052_Fig3_HTML.png?as=webp, \"Fine-tune training data\")\n",
        "\n",
        "( Contrastive language and vision learning of general fashion concepts)\n",
        "\n",
        "- hugging face : https://huggingface.co/patrickjohncyh/fashion-clip\n",
        "- paper : https://www.nature.com/articles/s41598-022-23052-9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWqL8PQ6HTdd"
      },
      "source": [
        "#### F-CLIP VS CLIP\n",
        "\n",
        "https://www.nature.com/articles/s41598-022-23052-9/tables/1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk00Q9joHTde"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00gVQZJTHTde"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model_name = \"patrickjohncyh/fashion-clip\"\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFZgFPolHTde"
      },
      "outputs": [],
      "source": [
        "# cropÎêú Ïù¥ÎØ∏ÏßÄÎì§Ïùò path Î∂àÎü¨Ïò§Í∏∞\n",
        "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
        "\n",
        "images = list(os.walk(cropped_path))[0][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhXK5KkTHTdf"
      },
      "outputs": [],
      "source": [
        "images[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9Q9gFzoHTdg"
      },
      "source": [
        "image embeddings from CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIZXaLQRHTdh"
      },
      "outputs": [],
      "source": [
        "from image_utils import extract_img_features\n",
        "\n",
        "img_emb = extract_img_features(img, processor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDSMrJ2oHTdh"
      },
      "outputs": [],
      "source": [
        "img_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD4mZOQJHTdj"
      },
      "source": [
        "text embeddings from CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l5PIaFLHTdj"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
        "\n",
        "model_name = \"patrickjohncyh/fashion-clip\"\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exc7dZyRHTdk"
      },
      "outputs": [],
      "source": [
        "def get_single_text_embedding(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors = \"pt\", padding=True)\n",
        "    text_embeddings = model.get_text_features(**inputs)\n",
        "    # convert the embeddings to numpy array\n",
        "    embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
        "    return embedding_as_np.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1WQ7K7FHTdk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et63DA5RHTdk"
      },
      "outputs": [],
      "source": [
        "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/3bccf2e618d8f5f51442037ad3c8d4fb.jpg\")\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1drmX0coHTdl"
      },
      "source": [
        "fashion fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDZ1YKo7HTdm"
      },
      "source": [
        "```json\n",
        "\"The text used is a concatenation of the highlight (e.g., ‚Äústripes‚Äù, ‚Äúlong sleeves‚Äù, ‚ÄúArmani‚Äù) and short description (‚Äú80s styled t-shirt‚Äù)) available in the Farfetch dataset.\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7y6aRnfHTdm"
      },
      "outputs": [],
      "source": [
        "img_emb = extract_img_features(img, processor, model)\n",
        "\n",
        "sample_texts = ['tshirt', \"formal suit and tie\",\n",
        "                'a woman', \"a lion in a cage\", \"black top short sleeves\",\n",
        "                'black shirt with check patterns, topwear', 'iphone']\n",
        "\n",
        "sample_texts_emb = get_single_text_embedding(sample_texts, model, tokenizer)\n",
        "\n",
        "sims = cosine_similarity(img_emb.cpu().detach().numpy(), sample_texts_emb)\n",
        "# Although dot product will be used in the future,\n",
        "print(\"Similarity with image\")\n",
        "for t, s in zip(sample_texts, sims[0]):\n",
        "    print(\"{} : {}\".format(t, s))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBSQvmt3HTdp"
      },
      "outputs": [],
      "source": [
        "img_emb.cpu().detach().numpy()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwU2hTVqHTdp"
      },
      "outputs": [],
      "source": [
        "np.array(s).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UETu8aMHHTdq"
      },
      "outputs": [],
      "source": [
        "print('dot product')\n",
        "for text, s in zip(sample_texts, sample_texts_emb):\n",
        "    sim = np.dot(img_emb.cpu().detach().numpy()[0], np.array(s))\n",
        "    print(text, sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32_vuffqHTdq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQamub6LHTdq"
      },
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "\n",
        "with open('img_embeddings_fashion_fine_tuned.json', 'r') as file:\n",
        "    for line in file:\n",
        "        # Convert each line to a dictionary\n",
        "        embedding_dict = json.loads(line.strip())\n",
        "\n",
        "        # Convert the list back to a NumPy array if necessary\n",
        "        for img_name, emb_list in embedding_dict.items():\n",
        "            embeddings[img_name] = np.array(emb_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE9UUsnxHTdr"
      },
      "outputs": [],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7QoMSYGHTdr"
      },
      "outputs": [],
      "source": [
        "type(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO6Adv27HTds"
      },
      "outputs": [],
      "source": [
        "for k,v in embeddings.items():\n",
        "    print(k)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACQDbKoHTds"
      },
      "outputs": [],
      "source": [
        "v.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aeeeb26"
      },
      "source": [
        "\"FashionCLIP performs the dot product between the input caption embedding and each image vector embedding\"\n",
        "\n",
        "\"The text used is a concatenation of the highlight (e.g., ‚Äústripes‚Äù, ‚Äúlong sleeves‚Äù, ‚ÄúArmani‚Äù) and short description (‚Äú80s styled t-shirt‚Äù)) available in the Farfetch dataset.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52e44dd0"
      },
      "source": [
        "```\n",
        "fashion fine-tuned model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75329654"
      },
      "source": [
        "```\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b121d0fa"
      },
      "source": [
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06943c35"
      },
      "source": [
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}