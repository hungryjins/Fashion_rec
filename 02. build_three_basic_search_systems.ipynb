{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "### 1. Building a VectorDB\n",
        "\n",
        "### 2. Text search\n",
        "\n",
        "### 3. Image search\n",
        "\n",
        "### 4. Hybrid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating a Local DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attributes = pd.read_csv(\"attribute_specific.csv\")\n",
        "df = pd.read_csv(\"clothes_final2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_read_f = list()\n",
        "\n",
        "with open(\"upsert_vectors_fashion_fine_tuned.json\", 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        data_read_f.append(data)\n",
        "\n",
        "print(f\"Successfully read {len(data_read_f)} fashion-fine-tuned CLIP embeddings from img_embeddings_fashion_fine_tuned.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['vdb_id'] = df['ImageId'].astype(str) + \"_\" + df['entity_id'].astype(str)\n",
        "df.drop(columns=['id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upsert_df_f = pd.DataFrame(data_read_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upsert_df_f.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d = pd.merge(df, upsert_df_f, left_on='vdb_id', right_on='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# d.to_csv(\"local_db.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = d['metadata'].values\n",
        "names = d['name'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_new = list()\n",
        "\n",
        "for n,m in zip(names, metadata):\n",
        "    m['category'] = n\n",
        "    metadata_new.append(m)\n",
        "\n",
        "d['metadata'] = metadata_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uploading to PineconeDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Convert the content to match the Pinecone upsert format.\n",
        "    - Upsert each category according to the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Upsert to pineconeDB!!\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"YOUR_PINECONE_API_KEY\")\n",
        "# Check the number of indexes\n",
        "# index_list = pc.list_indexes().indexes\n",
        "\n",
        "# index description\n",
        "index = pc.Index(\"fastcampus\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Max size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stored separately for each category\n",
        "# This is to save the index individually later\n",
        "\n",
        "# upsert!!\n",
        "def create_batches(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "df_categories = dict()\n",
        "\n",
        "for cat in tqdm(d['name'].unique()):\n",
        "    part_df = d.loc[d['name']==cat]\n",
        "    part_upserts = part_df[['id', 'values', 'sparse_values', 'metadata']].to_dict('records')\n",
        "    # Upsert in units of 100\n",
        "    df_categories[cat] = list(create_batches(part_upserts, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_categories.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upsert Format\n",
        "\n",
        "```json\n",
        "{\"id\" : \"0838a48a7b0bfa789a5181ab0e8f4ee2_3040\", # Image file name + entity ID\n",
        " \"values\" : [-0.08405803143978119, -0.7088879346847534, ...], # CLIP embeddings\n",
        " \"sparse_values\" : {\n",
        "    \"indices\" : [1045, 1062, ...], # non-zero index\n",
        "    \"values\" : [1.3038887977600098, 0.304147332906723, ...] # non-zero values\n",
        "    },\n",
        "\"metadata\" : {\n",
        "    # Image file path\n",
        "    \"img_path\": \"imaterialist-fashion-2020-fgvc7/cropped_images/0838a48a7b0bfa789a5181ab0e8f4ee2_3040.jpg\",\n",
        "    \"category\": \"coat\"\n",
        "} \n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for cat, batches in df_categories.items():\n",
        "#     print(cat)\n",
        "#     for batch in tqdm(batches):\n",
        "#         index.upsert(vectors=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Save each category to a separate index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Text to image search\n",
        "\n",
        "- Utilizing CLIP embedding\n",
        "    - Text and images are represented together in one vector space.\n",
        "    - Also, it is fine-tuned for the fashion dataset, making it more suitable for the current use case than a plain CLIP.\n",
        "    - The fine-tuned data is also trained based on various attributes of clothing (refer to the data below).\n",
        "\n",
        "![Fine-tune training data](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-022-23052-9/MediaObjects/41598_2022_23052_Fig3_HTML.png?as=webp, \"Fine-tune training data\")\n",
        "\n",
        "(Source: Contrastive language and vision learning of general fashion concepts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from image_utils import fetch_clip, draw_images\n",
        "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import gen_sparse_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from splade.splade.models.transformer_rep import Splade\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "# splade = 'naver/splade-v3'\n",
        "splade_model = Splade(splade_model_id, agg='max')\n",
        "splade_model.to('cpu')  # move to GPU if possible\n",
        "splade_model.eval()\n",
        "\n",
        "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_single_text_embedding(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors = \"pt\", padding=True)\n",
        "    text_embeddings = model.get_text_features(**inputs)\n",
        "    # convert the embeddings to numpy array\n",
        "    embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
        "    return embedding_as_np.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"Green dress with blue dots, long sleeve\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"nike\"\n",
        "\n",
        "# vans, nike, addidas\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"shoe\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"street fashion\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"Punk Fashion\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"Bohemian Fashion\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"flower patterns, short sleeve\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Advantages\n",
        "    - Categories such as famous brands, gender, clothing type, color, etc. can be specified as input without being manually defined.\n",
        "\n",
        "- Limitations\n",
        "    - Since it is a simple combination of attributes, it cannot recognize the characteristics of each part of the clothing.\n",
        "        - e.g.) Although \"blue dots\" was specified, a blue dress was expressed in the similarity.\n",
        "    - Abstract words such as street and bohemian fashion are combinations of various clothes.\n",
        "    (CLIP is trained using <clothing feature>-<clothing photo> pairs. Therefore, it does not match a category of fashion like \"street fashion\")\n",
        "\n",
        "- Overcoming measures\n",
        "    - Search that gives more weight to the characteristics of clothing by utilizing sparse vectors.\n",
        "    - If more abstract text is entered instead of clothing features, search the entire database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Image to image search\n",
        "\n",
        "- Utilizing CLIP embedding\n",
        "    - Although text and images are represented together in one vector space, Image-to-Image similarity measurement is possible.\n",
        "    - Also, it is fine-tuned for the fashion dataset, making it more suitable for the current use case than a plain CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from image_utils import extract_img_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test_image2.jpg\")\n",
        "\n",
        "img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "result = index.query(\n",
        "    vector=img_emb[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test_image.png\")\n",
        "\n",
        "img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "result = index.query(\n",
        "    vector=img_emb[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"shirt, blouse\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Limitations\n",
        "    - Since an image contains various elements such as the color of the clothes, people's poses, and light, it is not possible to select only the features of the clothes and conduct a search.\n",
        "    - In other words, there is a high possibility of overlooking the details of the clothes.\n",
        "- Overcoming measures\n",
        "    - Extract the features of the clothes from the image in text format, and convert them to a dense or sparse vector for searching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Hybrid search (Dense & sparse vector search)\n",
        "\n",
        "- Considering the characteristics of each part by utilizing splade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from splade.splade.models.transformer_rep import Splade\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "# splade = 'naver/splade-v3'\n",
        "sparse_model = Splade(sparse_model_id, agg='max')\n",
        "sparse_model.to('cpu')  # move to GPU if possible\n",
        "sparse_model.eval()\n",
        "\n",
        "splade_tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v-neck\n",
        "input_text = \"orange party dress with long sleeve, v neck\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "# sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
        "    # sparse_vector=sparse,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v-neck\n",
        "input_text = \"orange party dress with long sleeve, v neck\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
        "    sparse_vector=sparse,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])\n",
        "\n",
        "[i['id'] for i in result.matches]\n",
        "\n",
        "df.loc[df['vdb_id'].isin([i['id'] for i in result.matches]), ['vdb_id', 'ImageId', 'AttributesNames', 'second_AttributesNames']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[i['id'] for i in result.matches]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there is no text field related to fashion style, it is difficult to expect a significant performance improvement even if a sparse vector is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"Punk Fashion\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "# sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    # sparse_vector=sparse,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"Punk Fashion\"\n",
        "\n",
        "d = get_single_text_embedding(input_text, model, tokenizer)\n",
        "sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=d[0],\n",
        "    top_k=10,\n",
        "    sparse_vector=sparse,\n",
        "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Types of attributes that can be used\n",
        "```python\n",
        "list_of_attributes = ['main_category', 'silhouette', 'silhouette_fit', 'waistline', 'length',\n",
        "       'collar_type', 'neckline_type', 'sleeve_type', 'pocket_type',\n",
        "       'opening_type', 'non-textile material type', 'leather',\n",
        "       'textile finishing, manufacturing techniques', 'textile pattern']\n",
        "```\n",
        "<br>\n",
        "\n",
        "- Format of the document that can be expressed with attributes\n",
        "\n",
        "```json\n",
        "silhouette_name : symmetrical,\n",
        "collar_type_name : shirt (collar),\n",
        "opening_type_name : single breasted,\n",
        "non-textile material type_name : no non-textile material,\n",
        "textile finishing, manufacturing techniques_name : no special manufacturing technique,\n",
        "textile pattern_name : plain (pattern)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test_image4.png\")\n",
        "\n",
        "img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "result = index.query(\n",
        "    vector=img_emb,\n",
        "    top_k=5,  # how many results to return\n",
        "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test_image4.png\")\n",
        "\n",
        "img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "sparse_vector = gen_sparse_vector(\"suede jacket\", sparse_model, splade_tokenizer)\n",
        "\n",
        "result = index.query(\n",
        "    vector=img_emb,\n",
        "    sparse_vector=sparse_vector,\n",
        "    top_k=5,  # how many results to return\n",
        "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
