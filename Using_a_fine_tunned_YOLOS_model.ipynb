{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:19.035920Z",
     "start_time": "2023-06-13T18:30:19.035100Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:19.372956Z",
     "start_time": "2023-06-13T18:30:19.364345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here you should put the path of your image\n",
    "IMAGE_PATH = '0a4aae5ecd970a120bfcc6b377b6e187.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:19.588891Z",
     "start_time": "2023-06-13T18:30:19.578052Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the order of the categories list. NO NOT CHANGE. Just for visualization purposes\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:19.907746Z",
     "start_time": "2023-06-13T18:30:19.897407Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_channels(t):\n",
    "    \"\"\"\n",
    "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
    "    :param t: Tensor-like image\n",
    "    :return: Tensor-like image with three channels\n",
    "    \"\"\"\n",
    "    if len(t.shape) == 2:\n",
    "        return ToPILImage()(torch.stack([t for i in (0, 0, 0)]))\n",
    "    if t.shape[0] == 4:\n",
    "        return ToPILImage()(t[:3])\n",
    "    if t.shape[0] == 1:\n",
    "        return ToPILImage()(torch.stack([t[0] for i in (0, 0, 0)]))\n",
    "    return ToPILImage()(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:20.062255Z",
     "start_time": "2023-06-13T18:30:20.048776Z"
    }
   },
   "outputs": [],
   "source": [
    "def idx_to_text(i):\n",
    "    return cats[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:20.312733Z",
     "start_time": "2023-06-13T18:30:20.295385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random colors used for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        ax.text(xmin, ymin, idx_to_text(cl), fontsize=10,\n",
    "                bbox=dict(facecolor=c, alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.savefig(\"image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:20.578229Z",
     "start_time": "2023-06-13T18:30:20.572034Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.8):\n",
    "    # keep only predictions with confidence >= threshold\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "\n",
    "    # convert predicted boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "    # plot results\n",
    "    plot_results(image, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:20.797856Z",
     "start_time": "2023-06-13T18:30:20.787922Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:22.063798Z",
     "start_time": "2023-06-13T18:30:21.038746Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:23.048623Z",
     "start_time": "2023-06-13T18:30:22.064440Z"
    }
   },
   "outputs": [],
   "source": [
    "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image = image.resize((640, 640))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:28.074124Z",
     "start_time": "2023-06-13T18:30:23.048473Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = visualize_predictions(image, outputs, threshold=0.50)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T18:30:28.856962Z",
     "start_time": "2023-06-13T18:30:27.963754Z"
    }
   },
   "outputs": [],
   "source": [
    "img = visualize_predictions(image, outputs, threshold=0.50)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
