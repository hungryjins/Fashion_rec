{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# pandas dataframe display\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"YOUR_OPENAI_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "### 1. Object detection using yolo\n",
        "### 2. Bbox integration\n",
        "### 3. Cropping\n",
        "### 4. Determining if an image is searchable\n",
        "### 5. Providing search results for each detected category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attributes = pd.read_csv(\"attribute_specific.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pre-selected prediction labels\n",
        "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_PATH = 'test_images/test_image5.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
        "image = fix_channels(ToTensor()(image))\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_predictions(image, outputs, threshold=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "len(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def idx_to_text(i):\n",
        "    return cats[i]\n",
        "\n",
        "probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values > 0.5\n",
        "\n",
        "prob = probas[keep]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probas.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prob[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indices = [np.argmax(idx.detach().numpy()) for idx in prob]\n",
        "\n",
        "indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detected_cats = [cats[idx] for idx in indices]\n",
        "\n",
        "detected_cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(outputs.pred_boxes[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "boxes = outputs.pred_boxes[0, keep].cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bboxes_scaled = rescale_bboxes(boxes, image.size).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bboxes_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def plot_results_2(pil_img, labels, boxes):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for label, (xmin, ymin, xmax, ymax), c in zip(labels, boxes, colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        \n",
        "        ax.text(xmin, ymin, label, fontsize=10,\n",
        "                bbox=dict(facecolor=c, alpha=0.8))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results_2(image, detected_cats, bboxes_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### filter bounding boxes (select only necessary categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
        "new_df.name.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_of_interest = new_df.name.unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_of_interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_indices = list()\n",
        "keep_bboxes = list()\n",
        "\n",
        "for idx, box in zip(detected_cats, bboxes_scaled):\n",
        "    if idx in category_of_interest:\n",
        "        keep_indices.append(idx)\n",
        "        keep_bboxes.append(box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_indices, keep_bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### concat bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iou(boxA, boxB):\n",
        "    # Calculate the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # Compute the area of intersection\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "    # Compute the area of both the prediction and ground-truth rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "\n",
        "    # Compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the intersection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def merge_boxes(boxes, labels):\n",
        "    merged_boxes = []\n",
        "    merged_labels = []\n",
        "    used = set()\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        if i in used:\n",
        "            continue\n",
        "        current_box = boxes[i]\n",
        "        for j in range(i + 1, len(boxes)):\n",
        "            if j in used or labels[i] != labels[j]:\n",
        "                continue\n",
        "            if iou(current_box, boxes[j]) > 0.5:  # Assuming a positive IoU indicates overlap\n",
        "                # For xyxy format, we merge by finding the min and max coordinates\n",
        "                current_box = [\n",
        "                    min(current_box[0], boxes[j][0]), \n",
        "                    min(current_box[1], boxes[j][1]), \n",
        "                    max(current_box[2], boxes[j][2]), \n",
        "                    max(current_box[3], boxes[j][3])\n",
        "                ]\n",
        "                used.add(j)\n",
        "        merged_boxes.append(current_box)\n",
        "        merged_labels.append(labels[i])\n",
        "        used.add(i)\n",
        "\n",
        "    return np.array(merged_boxes), merged_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results_2(image, keep_indices, keep_bboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_bbox, merged_labels = merge_boxes(keep_bboxes, keep_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_labels, merged_bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def plot_results_2(pil_img, labels, boxes):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for label, (xmin, ymin, xmax, ymax), c in zip(labels, boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        \n",
        "        ax.text(xmin, ymin, label, fontsize=10,\n",
        "                bbox=dict(facecolor=c, alpha=0.8))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results_2(image, merged_labels, merged_bbox)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## crop images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from image_utils import crop_bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFilter\n",
        "\n",
        "def resize_img(image, category):\n",
        "    standard_size = {\"lowerbody\":[420, 540],\n",
        "        \"upperbody\":[500, 700],\n",
        "        \"wholebody\":[480, 880],\n",
        "        \"legs and feet\":[100, 150],\n",
        "        \"head\":[150, 100],\n",
        "        \"others\":[200, 350],\n",
        "        \"waist\":[200, 100],\n",
        "        \"arms and hands\":[75, 75],\n",
        "        \"neck\":[120, 200]}\n",
        "    \n",
        "    w, h = image.size\n",
        "    img_size = w*h\n",
        "\n",
        "    new_width, new_height = standard_size[category]\n",
        "    new_size = new_width * new_height\n",
        "\n",
        "    if img_size >= new_size:\n",
        "        # For downsizing\n",
        "        downsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "        return downsized_image\n",
        "    else:\n",
        "        # For upsizing\n",
        "        upsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "        upsized_image = upsized_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
        "        return upsized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = new_df[['supercategory', 'name']].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# categories.to_csv(\"categories.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here, by converting to a dictionary, each category is set as a key value.\n",
        "# Therefore, even if two 'shoes' are detected, only one is selected.\n",
        "cropped_images = dict()\n",
        "\n",
        "for label, box in zip(merged_labels, merged_bbox):\n",
        "    cropped = resize_img(crop_bbox(image, box), categories.loc[categories['name']==label, 'supercategory'].values[0])\n",
        "    cropped_images[label] = cropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search from DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"YOUR_PINECONE_API_KEY\")\n",
        "# Check the number of indexes\n",
        "# index_list = pc.list_indexes().indexes\n",
        "\n",
        "# index description\n",
        "index = pc.Index(\"fastcampus\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLIP\n",
        "from image_utils import fetch_clip, extract_img_features, draw_images\n",
        "\n",
        "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = dict()\n",
        "\n",
        "for label, image in cropped_images.items():\n",
        "    img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "    result = index.query(\n",
        "        vector=img_emb[0],\n",
        "        top_k=5,\n",
        "        filter={\"category\": {\"$eq\": label}},\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    paths = [i['metadata']['img_path'] for i in result.matches]\n",
        "\n",
        "    results[label] = paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, paths in results.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in paths])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clothes_detector(image, feature_extractor, model, thresh=0.5):\n",
        "    # all categories\n",
        "    cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', \n",
        "            'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', \n",
        "            'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', \n",
        "            'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', \n",
        "            'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']\n",
        "    # category we are interested in\n",
        "    category_of_interest = ['pants', 'shirt, blouse', 'jacket', 'top, t-shirt, sweatshirt', 'dress', 'shoe', 'glasses', \n",
        "                        'skirt', 'bag, wallet', 'belt', 'headband, head covering, hair accessory', 'sock', 'hat', \n",
        "                        'watch', 'glove', 'tights, stockings', 'sweater', 'tie', 'shorts', 'scarf', 'coat', 'vest', \n",
        "                        'umbrella', 'cardigan', 'cape', 'jumpsuit', 'leg warmer']\n",
        "    # yolo detection\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    # extract detected labels and boundingboxes\n",
        "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > thresh\n",
        "\n",
        "    prob = probas[keep]\n",
        "\n",
        "    indices = [np.argmax(idx.detach().numpy()) for idx in prob]\n",
        "    detected_cats = [cats[idx] for idx in indices]\n",
        "    boxes = outputs.pred_boxes[0, keep].cpu()\n",
        "\n",
        "    bboxes_scaled = rescale_bboxes(boxes, image.size).tolist()\n",
        "    \n",
        "    # keep boxes that we are interested in\n",
        "    keep_indices = list()\n",
        "    keep_bboxes = list()\n",
        "\n",
        "    for idx, box in zip(detected_cats, bboxes_scaled):\n",
        "        if idx in category_of_interest:\n",
        "            keep_indices.append(idx)\n",
        "            keep_bboxes.append(box)\n",
        "    # Integrate bboxes with overlapping sections\n",
        "    merged_bbox, merged_labels = merge_boxes(keep_bboxes, keep_indices)\n",
        "\n",
        "    # cropping\n",
        "    categories = pd.read_csv(\"categories.csv\")\n",
        "    cropped_images = dict()\n",
        "\n",
        "    for label, box in zip(merged_labels, merged_bbox):\n",
        "        cropped = resize_img(crop_bbox(image, box), categories.loc[categories['name']==label, 'supercategory'].values[0])\n",
        "        cropped_images[label] = cropped\n",
        "\n",
        "    return cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_search(index, cropped_images, model, processor, top_k=10):\n",
        "    results = dict()\n",
        "\n",
        "    for label, image in cropped_images.items():\n",
        "        img_emb = extract_img_features(image, processor, model).tolist()\n",
        "\n",
        "        result = index.query(\n",
        "            vector=img_emb[0],\n",
        "            top_k=top_k,\n",
        "            filter={\"category\": {\"$eq\": label}},\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        results[label] = result\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import clothes_detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test.jpg\")\n",
        "image = fix_channels(ToTensor()(image))\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_items = clothes_detector(image, feature_extractor, model, thresh=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_result = image_search(index, cropped_items, clip_model, clip_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the paths of the images\n",
        "paths = dict()\n",
        "for k,v in search_result.items():\n",
        "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
        "\n",
        "# Show the images\n",
        "for k,v in paths.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
