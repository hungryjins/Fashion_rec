{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import json\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Load necessary models and functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to pineconeDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"YOUR_PINECONE_API_KEY\")\n",
        "\n",
        "index = pc.Index(\"fastcampus\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLIP for generating dense vectors for images & text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from image_utils import fetch_clip, draw_images, extract_img_features\n",
        "\n",
        "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SPLADE for generating sparse vectors for text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from splade.splade.models.transformer_rep import Splade\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "splade_model = Splade(splade_model_id, agg='max')\n",
        "splade_model.to('cpu')\n",
        "splade_model.eval()\n",
        "\n",
        "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for implementing various search methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import fashion_query_transformer, clothes_detector, text_search, gen_sparse_vector, describe_clothes, additional_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "local_DB for two-stage search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_db = pd.read_csv(\"local_db.csv\")\n",
        "local_db['values'] = local_db['values'].apply(json.loads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLO for image detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy\n",
        "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
        "\n",
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "yolo_feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "yolo_model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pre-selected prediction labels\n",
        "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"YOUR_OPENAI_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import fashion_query_transformer, clothes_detector, get_top_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Text input only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_input = \"a black cat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gateway\n",
        "text_query = fashion_query_transformer(text_input)\n",
        "print(\"### Result from the text_input gateway : \\\n{}\".format(text_query))\n",
        "\n",
        "if text_query:\n",
        "    print(\"Searching ...\")\n",
        "    # text search\n",
        "    result = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=True)\n",
        "\n",
        "    # Get image paths\n",
        "    paths = dict()\n",
        "    for k,v in result.items():\n",
        "        paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
        "\n",
        "    # Show images\n",
        "    for k,v in paths.items():\n",
        "        print(k)\n",
        "        draw_images([Image.open(i) for i in v])\n",
        "else:\n",
        "    print(\"This text is not related to fashion. Please enter again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "def text_search(index, items_dict, model, tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=False):\n",
        "    search_results = dict()\n",
        "    for item in items_dict['items']:\n",
        "        text_emb = get_single_text_embedding(item['refined_text'], model, tokenizer)\n",
        "        if hybrid:\n",
        "            sparse_vector = gen_sparse_vector(item['refined_text'], splade_model, splade_tokenizer)\n",
        "        else:\n",
        "            sparse_vector=None\n",
        "        \n",
        "        if 'clothes_type' in list(item.keys()):\n",
        "            search_result = index.query(\n",
        "                            vector=text_emb[0],\n",
        "                            sparse_vector=sparse_vector,\n",
        "                            top_k=top_k,\n",
        "                            filter={\"category\": {\"$eq\": item['clothes_type']}},\n",
        "                            include_metadata=True\n",
        "                        )\n",
        "            search_results[item['clothes_type']] = search_result\n",
        "        else:\n",
        "            search_result = index.query(\n",
        "                            vector=text_emb[0],\n",
        "                            sparse_vector=sparse_vector,\n",
        "                            top_k=top_k,\n",
        "                            include_metadata=True\n",
        "                        )\n",
        "            search_results['all'] = search_result\n",
        "    return search_results\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Image input only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"test_images/test_image8.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(image_path)\n",
        "\n",
        "image = fix_channels(ToTensor()(image))\n",
        "# object detections\n",
        "print(\"Detecting items from the image.\")\n",
        "cropped_images = clothes_detector(image, yolo_feature_extractor, yolo_model, thresh=0.7)\n",
        "\n",
        "if len(cropped_images.keys())==0:\n",
        "    print(\"Nothing detected from the image\")\n",
        "else:\n",
        "    print(\"Detected \", cropped_images.keys())\n",
        "    \n",
        "    # describe the labels I have found\n",
        "    descriptions = dict()\n",
        "\n",
        "    print(\"Start creating descriptions for each item.\")\n",
        "    for i, img in cropped_images.items():\n",
        "        print(\"Created descriptions for {}\".format(i))\n",
        "        desc = describe_clothes(img, i, openai.api_key)\n",
        "        descriptions[i] = desc\n",
        "    print(\"\\nTransform the descriptions into structured query.\")\n",
        "    text_query = fashion_query_transformer(str(descriptions))\n",
        "    print(text_query)\n",
        "    results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100)\n",
        "    print(\"\\nRetrieved 100 images based on text search\")\n",
        "\n",
        "    print(\"\\nConducting additional search using the input images\")\n",
        "\n",
        "    results2 = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer, 10)\n",
        "\n",
        "    for k,v in results2.items():\n",
        "        print(k)\n",
        "        draw_images([Image.open(i) for i in v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Text and Image input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- If you enter a fashion style that is too different from the existing image, there is a high possibility that the desired result will not be obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_input = \"softer and more comfortable material\"\n",
        "image_path = \"test_images/test_image2.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_query = fashion_query_transformer(text_input)\n",
        "print(\"### Result from the text_input gateway : \\\n{}\".format(text_query))\n",
        "\n",
        "# Fashion-related query\n",
        "\n",
        "if 'clothes_type' in text_query['items'][0].keys():\n",
        "    print(\"Please enter the desired fashion style, rather than a specific item.\")\n",
        "elif text_query:\n",
        "    image = Image.open(image_path)\n",
        "    image = fix_channels(ToTensor()(image))\n",
        "    # object detections\n",
        "    print(\"Detecting items from the image.\")\n",
        "    cropped_images = clothes_detector(image, yolo_feature_extractor, yolo_model)\n",
        "\n",
        "    if len(cropped_images.keys())==0:\n",
        "        print(\"Nothing detected from the image\")\n",
        "    else:\n",
        "        print(\"Detected \", cropped_images.keys())\n",
        "        print(\"-\"*10, \"Start image only search\", \"-\"*10)\n",
        "        \n",
        "        # describe the labels I have found\n",
        "        descriptions = dict()\n",
        "\n",
        "        print(\"Start creating descriptions for each item.\")\n",
        "        for i, img in cropped_images.items():\n",
        "            print(\"Created descriptions for {}\".format(i))\n",
        "            desc = describe_clothes(img, i, openai.api_key)\n",
        "            descriptions[i] = desc\n",
        "        print(\"\\nTransform the descriptions into structured query.\")\n",
        "        text_query = fashion_query_transformer(str(descriptions))\n",
        "        print(text_query)\n",
        "        results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=200)\n",
        "        print(\"\\nRetrieved 200 images based on text search\")\n",
        "\n",
        "        print(\"\\nConducting additional search using the input images\")\n",
        "\n",
        "        results2 = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer, 100)\n",
        "        print(\"\\nRetrieved 100 items each, from sequential image search\")\n",
        "\n",
        "        print(\"-\"*10, \"Start reranking the results based on user input text\", \"-\"*10)\n",
        "        # Text search\n",
        "        new_results = list()\n",
        "\n",
        "        for k,v in results2.items():\n",
        "            ids = [os.path.splitext(os.path.basename(i))[0] for i in v]\n",
        "            tmp = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
        "\n",
        "            r = get_top_indices(tmp, text_query['items'][0]['refined_text'], k, clip_processor, clip_model, clip_tokenizer, 10, type='text')\n",
        "            new_results.append(r)\n",
        "\n",
        "        refined_result = dict()\n",
        "\n",
        "        for search_result in new_results:\n",
        "            category = list(search_result.keys())[0]\n",
        "            paths = list(search_result.values())[0]\n",
        "\n",
        "            full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
        "            refined_result[category] = full_paths\n",
        "        for k,v in refined_result.items():\n",
        "            print(k)\n",
        "            draw_images([Image.open(i) for i in v])\n",
        "else:\n",
        "    print(\"This text is not related to fashion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
