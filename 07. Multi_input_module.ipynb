{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import json\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "#### 1. Provide search results in the same way as image search\n",
        "#### 2. Rerank the results using additional input text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data & models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"YOUR_PINECONE_API_KEY\")\n",
        "\n",
        "index = pc.Index(\"fastcampus\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from image_utils import fetch_clip, draw_images, extract_img_features\n",
        "\n",
        "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from splade.splade.models.transformer_rep import Splade\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "splade_model = Splade(splade_model_id, agg='max')\n",
        "splade_model.to('cpu')\n",
        "splade_model.eval()\n",
        "\n",
        "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy\n",
        "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
        "\n",
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "yolo_feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "yolo_model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pre-selected prediction labels\n",
        "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import fashion_query_transformer, clothes_detector, text_search, gen_sparse_vector, describe_clothes, additional_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_db = pd.read_csv(\"local_db.csv\")\n",
        "local_db['values'] = local_db['values'].apply(json.loads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image and text input <br>: Item level sequential search (text embeddings + image embeddings) -> text search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Search for N relevant items based on the provided image.\n",
        "2. Rerank using a text search that specifies the fashion style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_input = \"I want the clothes to be more casual and easy to wear\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_PATH = 'test_images/test_image7.jpg'\n",
        "\n",
        "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
        "image = fix_channels(ToTensor()(image))\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"YOUR_OPENAI_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Image only search\n",
        "\n",
        "- Search for items related to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import clothes_detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_items = clothes_detector(image, yolo_feature_extractor, yolo_model, thresh=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cropped_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "descriptions = dict()\n",
        "\n",
        "for i, img in cropped_items.items():\n",
        "    print(i)\n",
        "    desc = describe_clothes(img, i, openai.api_key)\n",
        "    descriptions[i] = desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, v in descriptions.items():\n",
        "    print(i)\n",
        "    print(v)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_query = fashion_query_transformer(str(descriptions))\n",
        "text_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
        "results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = dict()\n",
        "for k,v in results.items():\n",
        "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
        "\n",
        "# Show images\n",
        "for k,v in paths.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v[:10]]) # Display only 10 items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve 20 to provide enough candidates for reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_results = additional_search(local_db, cropped_items, results, clip_processor, clip_model, clip_tokenizer, top_k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k,v in final_results.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Reranking using text embeddings\n",
        "\n",
        "- Rerank using the \"directionality\" of the clothing described in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, determine if the text mentions a specific item\n",
        "text_result = fashion_query_transformer(text_input)\n",
        "text_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from search_utils import get_top_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'clothes_type' not in text_result['items'][0].keys():\n",
        "    new_results = list()\n",
        "\n",
        "    for k,v in final_results.items():\n",
        "        # Retrieve the file_name again\n",
        "        ids = [os.path.splitext(os.path.basename(i))[0] for i in v]\n",
        "        tmp = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
        "\n",
        "        r = get_top_indices(tmp, text_result['items'][0]['refined_text'], k, clip_processor, clip_model, clip_tokenizer, 5, type='text')\n",
        "        new_results.append(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "refined_result = dict()\n",
        "\n",
        "for search_result in new_results:\n",
        "    category = list(search_result.keys())[0]\n",
        "    paths = list(search_result.values())[0]\n",
        "\n",
        "    full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
        "    refined_result[category] = full_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k,v in refined_result.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
