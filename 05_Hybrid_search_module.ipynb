{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungryjins/Fashion_rec/blob/main/05_Hybrid_search_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zeB-gvCHWcb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import base64\n",
        "import requests\n",
        "from PIL import Image\n",
        "import openai\n",
        "import os\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2s6Fx-qHWci"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "#### 1. Extract features using GPT4v to match our attributes -> Convert to text\n",
        "#### 2. Text search: top-100 (broad search)\n",
        "#### 3. Secondary search using image search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckEcGszIHWck"
      },
      "source": [
        "## Enrich image with descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KAJaTc-HWcl"
      },
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"openai_api_key\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuFVmilTHWcm"
      },
      "source": [
        "```python\n",
        "\"'main_category', 'silhouette', 'silhouette_fit', 'waistline',\n",
        "       'length', 'collar_type', 'neckline_type', 'sleeve_type',\n",
        "       'pocket_type', 'opening_type', 'non-textile material type',\n",
        "       'leather', 'textile finishing, manufacturing techniques',\n",
        "       'textile pattern', 'animal', 'other'\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooMohdYWHWcm"
      },
      "outputs": [],
      "source": [
        "from search_utils import clothes_detector\n",
        "from transformers import YolosFeatureExtractor, YolosForObjectDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwqZGVHSHWcn"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "\n",
        "image = Image.open(\"test_images/test_image5.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUqLu5qQHWcn"
      },
      "outputs": [],
      "source": [
        "cropped_images = clothes_detector(image, feature_extractor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8umSk7VHWco"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaABZC14HWco"
      },
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1nX59TiHWco"
      },
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPWwbsoSHWco"
      },
      "outputs": [],
      "source": [
        "cropped_images['shoe']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHh8Ge2oHWcp"
      },
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"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\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2LRnRImHWcp"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ8G1XKwHWcp"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGac1P4nHWcq"
      },
      "outputs": [],
      "source": [
        "# Read the image using GPT and create a description\n",
        "\n",
        "def describe_clothes(image, label, openai_key):\n",
        "  buffer = io.BytesIO()\n",
        "  # Save the image to the buffer in JPEG format\n",
        "  image.save(buffer, format=\"JPEG\")\n",
        "  buffer.seek(0)\n",
        "  image_data = buffer.read()\n",
        "\n",
        "  base64_image = base64.b64encode(image_data).decode('utf-8')\n",
        "  image_desc_prompt = \"\"\"Focus on {} inside the image.\n",
        "        Identify the attributes of the item.\n",
        "        The attributes you should answer are :\n",
        "        - clothes_type\n",
        "        - color\n",
        "        - silhouette\n",
        "        - silhouette_fit\n",
        "        - waistline\n",
        "        - sleeve_type\n",
        "        - collar_type\n",
        "        - length\n",
        "        - gender\n",
        "        - patterns\n",
        "        - textile_pattern\n",
        "\n",
        "        Ignore the attributes you cannot answer.\n",
        "        Keep the answer simple and clear, having max three words per attribute.\n",
        "  \"\"\".format(label)\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai_key}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4-vision-preview\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": image_desc_prompt\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 300\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "  return response.json()['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ_wr0AnHWcq"
      },
      "outputs": [],
      "source": [
        "cropped_images.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-cGA18tHWcr"
      },
      "outputs": [],
      "source": [
        "descriptions = dict()\n",
        "\n",
        "for i, img in cropped_images.items():\n",
        "    print(i)\n",
        "    desc = describe_clothes(img, i, openai.api_key)\n",
        "    descriptions[i] = desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAmPlcriHWcr"
      },
      "outputs": [],
      "source": [
        "descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nMLpLLWHWcr"
      },
      "outputs": [],
      "source": [
        "from search_utils import fashion_query_transformer, text_search\n",
        "from image_utils import draw_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwMi6Gp9HWcs"
      },
      "source": [
        "Convert text in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTEGFD3RHWcs"
      },
      "outputs": [],
      "source": [
        "text_query = fashion_query_transformer(str(descriptions))\n",
        "text_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KE0y1YxHWcs"
      },
      "outputs": [],
      "source": [
        "from splade.splade.models.transformer_rep import Splade\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
        "\n",
        "splade_model = Splade(splade_model_id, agg='max')\n",
        "splade_model.to('cpu')\n",
        "splade_model.eval()\n",
        "\n",
        "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST4kXA9eHWcs"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
        "# index number check\n",
        "# index_list = pc.list_indexes().indexes\n",
        "\n",
        "# index description\n",
        "index = pc.Index(\"fastcampus\")\n",
        "# index.describe_index_stats()\n",
        "\n",
        "# CLIP\n",
        "from image_utils import fetch_clip, extract_img_features, draw_images\n",
        "\n",
        "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMtSqGbhHWct"
      },
      "outputs": [],
      "source": [
        "results = text_search(index, text_query, model, tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
        "results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTLrfnG_HWct"
      },
      "outputs": [],
      "source": [
        "len(results['tights, stockings'].matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYUVPAbMHWct"
      },
      "outputs": [],
      "source": [
        "paths = dict()\n",
        "for k,v in results.items():\n",
        "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
        "\n",
        "# show images\n",
        "for k,v in paths.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v[:10]]) # display only 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btyknl5gHWcu"
      },
      "source": [
        "### Perform another search within the matching images (image search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54LZaCk-HWcu"
      },
      "outputs": [],
      "source": [
        "local_db = pd.read_csv(\"local_db.csv\")\n",
        "local_db['values'] = local_db['values'].apply(json.loads)\n",
        "local_db.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIIvIntdHWcu"
      },
      "outputs": [],
      "source": [
        "local_db.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drY2PwXkHWcv"
      },
      "source": [
        "Save search result values by each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV7U9TIGHWcv"
      },
      "outputs": [],
      "source": [
        "results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L0zpW6JHWcw"
      },
      "outputs": [],
      "source": [
        "ids = list()\n",
        "\n",
        "for category, value in results.items():\n",
        "    id = [i['id'] for i in value['matches']]\n",
        "    ids.append({category:id})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kVS4ti9HWcx"
      },
      "outputs": [],
      "source": [
        "ids[0].keys(), list(ids[0].values())[0][:3], \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-BfDeHWHWcy"
      },
      "outputs": [],
      "source": [
        "ids[1]['shoe']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyPyQBjhHWcz"
      },
      "outputs": [],
      "source": [
        "ids[0]['tights, stockings']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8JU-GSQHWc1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzxjmW2CHWc3"
      },
      "outputs": [],
      "source": [
        "# Convert image to embedding\n",
        "# cropped_images is also needed\n",
        "\n",
        "final_results = list()\n",
        "\n",
        "for search_result in ids:\n",
        "    category = list(search_result.keys())[0]\n",
        "    search_ids = list(search_result.values())[0]\n",
        "    # Get relevant items\n",
        "    filtered_local_db = local_db.loc[local_db['vdb_id'].isin(search_ids)]\n",
        "\n",
        "    img_emb = extract_img_features(cropped_images[category], processor, model)\n",
        "\n",
        "    # def search_local_db()\n",
        "    def calculate_dot_products(embedding, df, column_name):\n",
        "        dot_products = df[column_name].apply(lambda x: np.dot(embedding, x))\n",
        "        return dot_products\n",
        "\n",
        "    # Calculate dot products\n",
        "    dot_products = calculate_dot_products(img_emb.cpu().numpy()[0], filtered_local_db, 'values')\n",
        "\n",
        "    # Find the indices of the top 5 most similar embeddings\n",
        "    top_indices = dot_products.nlargest(10).index\n",
        "\n",
        "    # Retrieve the top 5 most similar embeddings\n",
        "    top_similar_ids = filtered_local_db.loc[top_indices, 'vdb_id'].tolist()\n",
        "\n",
        "    final_results.append({category:top_similar_ids})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtCbT-3RHWc4"
      },
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LJ7FT2oHWc6"
      },
      "outputs": [],
      "source": [
        "filtered_local_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UffGEb9EHWc7"
      },
      "outputs": [],
      "source": [
        "final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_xqoYcWHWc8"
      },
      "outputs": [],
      "source": [
        "for search_result in final_results:\n",
        "    category = list(search_result.keys())[0]\n",
        "    paths = list(search_result.values())[0]\n",
        "\n",
        "    full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
        "    print(category)\n",
        "    draw_images([Image.open(i) for i in full_paths])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UxgitmbHWc8"
      },
      "source": [
        "## Converting to Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMqnY8G8HWc9"
      },
      "outputs": [],
      "source": [
        "from search_utils import get_single_text_embedding\n",
        "\n",
        "def calculate_dot_products(embedding, df, column_name):\n",
        "    dot_products = df[column_name].apply(lambda x: np.dot(embedding, x))\n",
        "    return dot_products\n",
        "\n",
        "\n",
        "def get_top_indices(db, input_data, category, clip_processor, clip_model, clip_tokenizer, top_k, type='image'):\n",
        "    if type=='image':\n",
        "        # input_data should be a single cropped image\n",
        "        emb = extract_img_features(input_data, clip_processor, clip_model)\n",
        "        # Calculate dot products\n",
        "        dot_products = calculate_dot_products(emb.cpu().numpy()[0], db, 'values')\n",
        "    elif type=='text':\n",
        "        # input_data should be a single string of text\n",
        "        emb = get_single_text_embedding(input_data, clip_model, clip_tokenizer)\n",
        "        # Calculate dot products\n",
        "        dot_products = calculate_dot_products(np.array(emb)[0], db, 'values')\n",
        "\n",
        "    # Find the indices of the top 5 most similar embeddings\n",
        "    top_indices = dot_products.nlargest(top_k).index\n",
        "\n",
        "    # Retrieve the top 5 most similar embeddings\n",
        "    top_similar_ids = db.loc[top_indices, 'vdb_id'].tolist()\n",
        "\n",
        "    return {category:top_similar_ids}\n",
        "\n",
        "\n",
        "def additional_search(local_db, cropped_images, search_results, clip_processor, clip_model, clip_tokenizer, top_k=10):\n",
        "\n",
        "    ids = list()\n",
        "    for category, value in search_results.items():\n",
        "        id = [i['id'] for i in value['matches']]\n",
        "        ids.extend(id)\n",
        "\n",
        "    final_results = list()\n",
        "\n",
        "    # From the overall items, retrieve only the ones that were retrieved in the first search\n",
        "    db = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
        "\n",
        "    for label, v in search_results.items(): # From text\n",
        "        tmp = db.loc[db['name']==label]\n",
        "\n",
        "        # If the label exists in both text and image\n",
        "        if label in cropped_images.keys():\n",
        "            r = get_top_indices(tmp, cropped_images[label], label, clip_processor, clip_model, clip_tokenizer, top_k, type='image')\n",
        "            final_results.append(r)\n",
        "        # If the label exists only in text, just get top_k\n",
        "        else:\n",
        "            final_results.append({ label : [i['id'] for i in v['matches']][:top_k]} )\n",
        "\n",
        "    refined_result = dict()\n",
        "\n",
        "    for search_result in final_results:\n",
        "        category = list(search_result.keys())[0]\n",
        "        paths = list(search_result.values())[0]\n",
        "\n",
        "        full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
        "        refined_result[category] = full_paths\n",
        "\n",
        "\n",
        "    return refined_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBG2YTaRHWc-"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAwb0pZtHWc-"
      },
      "outputs": [],
      "source": [
        "# initialize openai\n",
        "os.environ['OPENAI_API_KEY']= \"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\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGyfOAFHWc-"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "\n",
        "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmUGLF_SHWc-"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5rb1dUoHWc_"
      },
      "outputs": [],
      "source": [
        "from search_utils import clothes_detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmsg2AqqHWc_"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"test_images/test.jpg\")\n",
        "# image = fix_channels(ToTensor()(image))\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm3CHrtyHWc_"
      },
      "outputs": [],
      "source": [
        "cropped_images = clothes_detector(image, feature_extractor, model, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJANXuUyHWdA"
      },
      "outputs": [],
      "source": [
        "cropped_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRk4lho7HWdA"
      },
      "outputs": [],
      "source": [
        "descriptions = dict()\n",
        "\n",
        "for i, img in cropped_images.items():\n",
        "    print(i)\n",
        "    desc = describe_clothes(img, i, openai.api_key)\n",
        "    descriptions[i] = desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3w88cHmHWdA"
      },
      "outputs": [],
      "source": [
        "for i, v in descriptions.items():\n",
        "    print(i)\n",
        "    print(v)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAFKrWoRHWdB"
      },
      "outputs": [],
      "source": [
        "text_query = fashion_query_transformer(str(descriptions))\n",
        "text_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6F93OUHWdB"
      },
      "outputs": [],
      "source": [
        "results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
        "results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFG8FvngHWdB"
      },
      "outputs": [],
      "source": [
        "paths = dict()\n",
        "for k,v in results.items():\n",
        "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
        "\n",
        "# show images\n",
        "for k,v in paths.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v[:10]]) # display only 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13HaFdriHWdC"
      },
      "outputs": [],
      "source": [
        "final_results = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVU_9ZYFHWdC"
      },
      "outputs": [],
      "source": [
        "for k,v in final_results.items():\n",
        "    print(k)\n",
        "    draw_images([Image.open(i) for i in v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfsX55oJHWdC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fastcampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}